
\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{float}

\title{Determining the Overhead of the EZTrace System}
\author{Philip M. Westrich}
\date{May 1, 2017}

\begin{document}

\maketitle
\vspace{-0.3in}\noindent\rule{\linewidth}{0.4pt}
\doublespacing

\begin{abstract}

    High performance computing plays an integral role in modern science. Many will use large scale simulations to test 
    things from new theories in astrophysics to determining the next week's weather. These simulations depend on the 
    programs written to carry them out; if they are incorrect or inefficient, they may possibly hinder their progress 
    rather than help it.
    
    The people who run these simulations always would like to make them faster; no one likes long wait times. Two methods 
    used to determine the inefficiencies in large parallel or distributed systems are tracing and logging. However, they 
    both do not come for free. On the extreme end, parallel debuggers can slow down a program up to 1000 times!
    
    In this paper, we will test the tracing system EZTrace and determine how much of a performance penalty it introduces 
    into a system.\\
 
\end{abstract}

\vspace{-0.3in}\noindent\rule{\linewidth}{0.4pt}
\pagebreak

\section{Introduction}

Large scale simulations are used all over the place to model hard to test phenomenon in modern science. As these models 
become more complex and the simulations used to predict according to them become more and more complex, they require more 
computational power, time, or both to produce accurate enough results.

In order to prevent resources from being wasted, it is important that the algorithms used are as efficient as possible. 
The difference between $ O(n^{2.5}) $ and $ O(n^3) $ in computation or communication could mean hundreds of hours per 
simulation wasted.

\subsection{Tracing and profiling}

Two related methods used to discern the reasons behind poor performance are tracing and profiling. They have been used 
for decades to discern the reasons a program is either producing incorrect results or operating inefficiently.

Program tracing is the process of annotating a program with code such that it records the sequence of defined blocks a 
program executes, and running the modified program to collect this data into a trace file. Program profiling counts the 
number of times those defined blocks are executed in a program's lifetime. \cite{Ball1994, Larus1993} 

However, tracing and profiling do not come for free. Since they usually involve inserting extra code into a program to 
execute, there is naturally some nonzero amount of overhead introduced that increases as the number of blocks to measure 
increases. The data generated from the process can and oftentimes quickly grows to such a size that it either physically 
cannot be stored on available storage, or it becomes impractical to sort through for any meaning. Therefore, it is 
imperative that the systems that perform the tracing or profiling are as efficient as possible with both time and storage 
space. \cite{Ball1994, Larus1993, Mohror2012}

Tools have been written that can do tracing and profiling in a decently efficient manner, but there are still some limitations 
to what they can do. In the parallel computing world, there are numerous tools that can profile standard libraries, such as 
OpenMP, MPI, POSIX threads, and others. However, when it comes to instrumenting user-defined functions, those tools fall 
short, and the developer must resort to manual instrumentation. They also do not allow for different interpretation of 
events; once an even is written, some data is lost and there is no other way to interpret it. \cite{Trahay2011} 

\subsection{The EZTrace system}

These complex systems require complicated tools to analyze them, especially once parallel and distributed computing are 
introduced. One solution presented was the EZTrace performance analysis framework. The authors of the framework state 
that they have written EZTrace with the aforementioned issues in mind. \cite{Trahay2011}

The EZTrace system is plugin-based. It comes with several preinstalled plugins for analysis of the common parallel and 
distributed programming libraries: OpenMP, MPI, POSIX threads, and the PLASMA library. It also gives a method for a 
developer to define their own to handle other code in their program. This way, most parellel systems can be automatically 
analyzed with minimal effort from the developer, but if they decide more is needed, they can write a plugin. \cite{Trahay2011}

The EZTrace system has a two-phase system. In the first, while the program is running, events are recorded. In the second, 
after the program is finished, these events are then interpreted. This allows for decoupled recording and interpretation; 
the user can treat an event in whichever way they wish depending on other context. It also means that more expensive 
operations can be placed in the second phase, while the first focuses only on data retention. \cite{Trahay2011}

In their paper, the authors claim that the EZTrace system performs all the aforementioned tasks in an efficient manner. 
In this paper, we intend to duplicate their results.

\section{Testing methods}

In their paper, the authors of the EZTrace system used the NAS Parallel Benchmark suite \cite{Bailey1991} to show that the 
overhead their system introduced was indeed minimal compared to other tracing systems. For this paper, we ran the same 
benchmarks at the same sizes and number of threads as they did. Since the resources were available, we ran them at a 
larger size as well. \cite{Trahay2011}

The partition of the high performance cluster that was used consists of four nodes, each with dual Intel Xeon E5-2680v4 
and 128 GB of RAM, and was running Debian 7.10. Each benchmark ran was submitted to the batch queuing system and was 
restricted to those nodes. Each test was run on a single node by itself.

The latest version of the benchmarks at the time of writing this paper (3.3.1) were downloaded and compiled on the compute 
nodes for size classes B, C, and D, except for the FT benchmark, which could not be compiled for size D. Benchmarks that 
required a perfect square number of processes were built for 36 processes, and all others were built for 32. EZTrace was 
installed from the Debian repository, which gave version 0.7-2-4. EZTrace was told to trace the MPI calls from each benchmark.

\section{Results}

The results from the tests are in the following tables.

\begin{table}[H]
\centering
\caption{Execution time (s), without EZTrace}
\label{tab:notrace}
\begin{tabular}{|l|cccccccc|}
\hline
Class & BT      & CG     & EP     & FT   & IS    & LU      & MG     & SP      \\ \hline
B     & 22.07   & 5.28   & 3.04   & 5.46 & 0.45  & 14.54   & 1.36   & 26.42   \\
C     & 91.72   & 19.87  & 10.53  & 21.8 & 1.61  & 56.25   & 7.33   & 104.88  \\
D     & 1655.87 & 710.35 & 146.11 &      & 25.33 & 1330.85 & 170.91 & 2046.87 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Execution time (s), with EZTrace}
\label{tab:eztrace}
\begin{tabular}{|l|cccccccc|}
\hline
Class & BT      & CG     & EP     & FT    & IS    & LU      & MG     & SP      \\ \hline
B     & 21.88   & 7.19   & 3.08   & 5.53  & 0.53  & 15.25   & 1.38   & 23.40   \\
C     & 89.27   & 18.43  & 10.07  & 20.28 & 1.80  & 66.12   & 8.39   & 106.32  \\
D     & 1654.77 & 779.60 & 147.66 &       & 27.79 & 1320.66 & 172.21 & 2207.90 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{EZTrace overhead}
\label{tab:overhead}
\begin{tabular}{|l|cccccccc}
\hline
Class & BT      & CG      & EP      & FT      & IS      & LU      & MG      & SP       \\ \hline
B     & -0.86\% & 36.17\% & 1.32\%  & 1.28\%  & 17.78\% & 4.88\%  & 1.47\%  & -11.43\% \\
C     & -2.67\% & -7.25\% & -4.37\% & -4.25\% & 11.80\% & 17.55\% & 14.46\% & 1.37\%   \\
D     & -0.07\% & 9.75\%  & 1.06\%  &         & 9.71\%  & -0.77\% & 0.76\%  & 7.87\%   \\
\hline
\end{tabular}
\end{table}

\subsection{Discussion}

As can be seen from \ref{tab:overhead}, there is no clear-cut trend in the amount of overhead as the problem size increases.

\section{Conclusion}

This is where I sum everything up.

\bibliographystyle{acm}
\bibliography{bibliography.bib}

\end{document}
